# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math
import torch
from functools import cache
from logging import getLogger
from collections import deque
import torch.distributed as dist


logger = getLogger()


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def apply_masks(x, masks):
    """
    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]
    :param masks: list of tensors containing indices of patches [0,N) to keep
    """
    all_x = []
    for m in masks:
        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))
        all_x += [torch.gather(x, dim=1, index=mask_keep)]
    return torch.cat(all_x, dim=0)


def repeat_interleave_batch(x, B, repeat):
    N = len(x) // B
    x = torch.cat([
        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)
        for i in range(N)
    ], dim=0)
    return x


_RANKME_ACCUMULATE = 32
_RANKME_EPSILON = 1e-7

class RankMe():
    def __init__(self, limit: int = _RANKME_ACCUMULATE, epsilon: float=_RANKME_EPSILON):
        self.limit = limit
        self.bounded_queue = deque(maxlen=self.limit)
        self.epsilon = epsilon

    def enqueue(self, encoding: torch.Tensor) -> float:
        with torch.no_grad():
            world_size = dist.get_world_size()
            batch_size, *_ = encoding.shape

            gathered_encodings = [torch.zeros_like(encoding) for _ in range(world_size)]
            dist.all_gather(gathered_encodings, encoding)

            full_batch = torch.cat(gathered_encodings, dim=0)
            self.bounded_queue.append(full_batch)

            if len(self.bounded_queue) > 0:
                queue_batch = torch.cat(list(self.bounded_queue), dim=1)
                score = self.calculate_rankme(queue_batch, self.epsilon)
                # NOTE that all devices will have the same data at this point so no need for any allreduce/allgather
                return score
    
    @classmethod
    def calculate_rankme(cls, x: torch.Tensor, epsilon: float) -> float:
        with torch.no_grad():
            _u, s, _vh = torch.linalg.svd(x, full_matrices=False)
            p = (s / torch.sum(s, axis=0)) + epsilon
            entropy = -torch.sum(p * torch.log(p))
            return torch.exp(entropy).item()


@cache
def rankme() -> RankMe:
    logger.info(f'Initialized distributed RankMe.')
    return RankMe(limit=_RANKME_ACCUMULATE, epsilon=_RANKME_EPSILON)